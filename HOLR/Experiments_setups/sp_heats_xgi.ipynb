{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle \n",
    "\n",
    "sys.path.append(sys.path[0] + \"/..\")  # Adds higher directory to python modules path.\n",
    "\n",
    "import numpy as np\n",
    "from Functions import renormalize, scomplex, plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import xgi\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "\n",
    "\n",
    "colors = [\"#003F5C\",\"#2F4B7C\",\"#665191\",\"#A05195\",\"#D45087\",\"#F95D6A\",\"#FF7C43\",\"#FFA600\"]\n",
    "colors_sequential = colors + colors + colors + colors \n",
    "colors = [\"#02405c\", \"#fea600\", \"#a20655\", \"#5e96c5\", \"#4b3596\", \"#fa6ca9\", \"#8d6cf6\"]\n",
    "colors_curves = colors+ colors+ colors+ colors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_of_order_hg(sc,k,l):\n",
    "    # sc: hypergraph object\n",
    "    # k: order of the diffusing simplices\n",
    "    # l: order of the interaction simplices\n",
    "\n",
    "    keys = [\"nodes\", \"edges\", \"faces\", \"tetrahedra\", \"4-simplices\"]\n",
    "    nk = sc[f\"n{k}\"]\n",
    "    adj = np.zeros((nk,nk),dtype = int)\n",
    "\n",
    "    if l < k: \n",
    "       \n",
    "        diff_units = sc[keys[k]]\n",
    "        \n",
    "        if l == 0:\n",
    "            for i in range(nk):\n",
    "                for j in range(i+1,nk):\n",
    "                    intersection = (set(diff_units[i,:]) & set(diff_units[j,:]))\n",
    "                    adj[i,j] = 2*len(intersection)\n",
    "                    \n",
    "        else: \n",
    "            edge_dict, face_dict, tet_dict = scomplex.make_dict(sc)\n",
    "            dicts = [{(i,):i for i in range(sc[\"n0\"])},edge_dict,face_dict,tet_dict]\n",
    "            for i in range(nk):\n",
    "                for j in range(i+1,nk):\n",
    "                    intersection = (set(diff_units[i,:]) & set(diff_units[j,:]))\n",
    "                    combs = list(combinations(intersection, l+1))\n",
    "                    for c in combs:\n",
    "                        if c in dicts[l]:\n",
    "                            adj[i,j] += 2\n",
    "                \n",
    "\n",
    "    elif l > k:\n",
    "        edge_dict, face_dict, tet_dict = scomplex.make_dict(sc)\n",
    "        dicts = [{(i,):i for i in range(sc[\"n0\"])},edge_dict,face_dict,tet_dict]\n",
    "        for i,simp in enumerate(sc[keys[l]]):\n",
    "            combs = list(combinations(simp, k+1))\n",
    "            ncombs = len(combs)\n",
    "            combs_present = []\n",
    "            for c in range(ncombs):\n",
    "                if combs[c] in dicts[k]:\n",
    "                    combs_present.append(dicts[k][combs[c]])\n",
    "\n",
    "            for c1 in combs_present:\n",
    "                for c2 in combs_present:\n",
    "                    if c2 != c1:\n",
    "                        adj[c1,c2] += 1\n",
    "\n",
    "    return (adj + adj.T)//2\n",
    "\n",
    "def XO_laplacian_hg(sc,k,l):\n",
    "    A = adjacency_of_order_hg(sc,k,l)\n",
    "    K = np.sum(A, 0)\n",
    "    L = np.diag(K) - A\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def induce_simplices_hg(sc, mapnodes):\n",
    "    # Finds induced simplices in the simplicial complex after its nodes are coarse grained \n",
    "    # INPUTS\n",
    "    # sc: simplicial complex object\n",
    "    # mapnodes: mapping from each node in sc to the label of its signature\n",
    "  \n",
    "    # OUTPUTS\n",
    "    # new_sc: coarse grained simplicial complex object\n",
    "\n",
    "    keys = [\"edges\", \"faces\", \"tetrahedra\", \"4-simplices\"]\n",
    "    new_sc = {\n",
    "        \"nodes\": np.sort(np.unique(mapnodes)),\n",
    "    }\n",
    "    new_sc[\"n0\"] = len(new_sc[\"nodes\"])\n",
    "    new_sc[\"nodes\"] = np.reshape(new_sc[\"nodes\"], (new_sc[\"n0\"], 1))\n",
    "    for key in keys:\n",
    "        new_sc[key] = []\n",
    "\n",
    "    # Connect supernodes with hyperedges\n",
    "    for order, key in enumerate(keys):\n",
    "        for i in range(sc[f\"n{order+1}\"]):\n",
    "            nodes = mapnodes[sc[key][i, :]]\n",
    "            un = np.unique(nodes)\n",
    "            lun = len(un)\n",
    "            if lun > 1:\n",
    "                new_sc[keys[lun-2]].append(un)\n",
    "\n",
    "    # Remove duplicate hyperedges\n",
    "    for order, key in enumerate(keys):\n",
    "        if len(new_sc[key]) != 0:\n",
    "            new_sc[key] = np.unique(\n",
    "                np.sort(np.array(new_sc[key], dtype=int), axis=1), axis=0\n",
    "            )\n",
    "            new_sc[f\"n{order+1}\"] = new_sc[key].shape[0]\n",
    "        else:\n",
    "            new_sc[key] = np.zeros((0, order+2), dtype=int)\n",
    "            new_sc[f\"n{order+1}\"] = 0\n",
    "\n",
    "    return new_sc\n",
    "\n",
    "def renormalize_single_step_hg(sc,tau, diff_order =0, int_order = 1, D = None, U = None, VERBOSE = True):\n",
    "    # Performs a single step of higher-order Laplacian renormalization \n",
    "    # INPUTS\n",
    "    # sc: simplicial complex object\n",
    "    # tau: diffusion time\n",
    "    # diff_order: order of the diffusing simplices\n",
    "    # int_order: order of the interaction simplices\n",
    "    # D: the list of Laplacian eigenvlaues, if None computes them from scratch\n",
    "    # U: the list of Laplacian eigenvectors, if None computes them from scratch \n",
    "    # VERBOSE: if True print the number of nodes after the coarse-graining\n",
    "\n",
    "    # OUTPUTS\n",
    "    # new_sc: renormalized simplicial complex\n",
    "    # mapnodes: array associating to each node in sc the node in new_sc it is mapped to\n",
    "    # clusters: cluster label of each simplex of order diff_order\n",
    "  \n",
    "\n",
    "    if (D is None) or (U is None):\n",
    "        L = XO_laplacian_hg(sc, diff_order, int_order)\n",
    "        D,U = np.linalg.eigh(L)\n",
    "\n",
    "    rho  = np.abs(U@np.diag(np.exp(-tau*D))@U.T)\n",
    "\n",
    "    Gv = nx.Graph()\n",
    "    Gv.add_nodes_from([i for i in range(sc[f\"n{diff_order}\"])])\n",
    "    for i in range(sc[f\"n{diff_order}\"]):\n",
    "        for j in range(i+1,sc[f\"n{diff_order}\"]):\n",
    "            if rho[i,j] >= min(rho[i,i],rho[j,j]):\n",
    "                Gv.add_edge(i,j)\n",
    "\n",
    "        \n",
    "    idx_components = {u:i for i,node_set in enumerate(nx.connected_components(Gv)) for u in node_set}\n",
    "    clusters = [idx_components[u] for u in Gv.nodes]\n",
    "\n",
    "    mapnodes,__ = renormalize.coarse_grain(sc,diff_order,clusters,np.max(clusters)+1)\n",
    "    new_sc = induce_simplices_hg(sc, mapnodes)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(new_sc[\"n0\"])\n",
    "\n",
    "        \n",
    "    return new_sc, mapnodes, clusters  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hg_to_sc(H):\n",
    "    H.cleanup()\n",
    "\n",
    "    sc = {}\n",
    "    sc[\"nodes\"] = np.sort(np.array([H.nodes]).T,0)\n",
    "    sc[\"n0\"] = sc[\"nodes\"].shape[0]\n",
    "    keys = [\"edges\",\"faces\",\"tetrahedra\",\"4-simplices\"]\n",
    "\n",
    "    for k in keys:\n",
    "        sc[k] = []\n",
    "    for e in H.edges.members():\n",
    "        if len(e) <= 5:\n",
    "            sc[keys[len(e)-2]].append(list(e))\n",
    "\n",
    "    for i,k in enumerate(keys):\n",
    "        if len(sc[k]) == 0:\n",
    "            sc[k] = np.zeros((0,i+2))\n",
    "        else:\n",
    "            sc[k] = np.unique(np.sort(np.array(sc[k]),1),axis =0)\n",
    "        \n",
    "        sc[f\"n{i+1}\"] = sc[k].shape[0] \n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacians(dim = 2):\n",
    "    if dim == 4:\n",
    "        laps = [\"01\",\"02\",\"03\",\"04\",\"10\",\"12\",\"13\",\"14\",\"20\",\"21\",\"23\",\"24\",\"30\",\"31\",\"32\",\"34\",\"40\",\"41\",\"42\",\"43\"]\n",
    "    elif dim == 3:\n",
    "        laps = [\"01\",\"02\",\"03\",\"10\",\"12\",\"13\",\"20\",\"21\",\"23\",\"30\",\"31\",\"32\"]\n",
    "    elif dim == 2:\n",
    "        laps = [\"01\",\"02\",\"10\",\"12\",\"20\",\"21\"]\n",
    "    elif dim == 1:\n",
    "        laps = [\"01\",\"10\"]\n",
    "    return laps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGI Hypergraphs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "#xgi.load_xgi_data()\n",
    "\n",
    "#select hypergraph with max HOI = 5 nodes\n",
    "names_hg = ['congress-bills', 'contact-high-school', 'contact-primary-school', 'diseasome', 'disgenenet','email-enron', 'email-eu', 'hospital-lyon', 'house-bills', 'house-committees', 'hypertext-conference', 'invs13', 'invs15','kaggle-whats-cooking', 'malawi-village','ndc-classes', 'ndc-substances', 'science-gallery', 'senate-bills', 'senate-committees','sfhh-conference']\n",
    "true_names_hg = ['US Congress Bills', 'High school contacts', 'Primary school contact', 'Diseasome', 'Disgenenet', 'Enron email', 'Eu email', 'Hospital contacts', 'House bills', 'House committees', 'Hypertext contacts', 'InVS13', 'InVS15', 'Kaggle Competition', 'Malawi contacts', 'Classes of NDC', 'Substances of NDC', 'Science Gallery', 'Senate bills', 'Senate committees', 'SFHH conference']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Import hypergraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgi datasets\n",
    "save = False\n",
    "if save == True:\n",
    "    #save the hypergraphs\n",
    "    with open(\"../Experiments_results/xgi/xgi_sc.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scs, f)\n",
    "else:\n",
    "    #import the hypergraphs in our format\n",
    "    with open(\"../Experiments_results/xgi/xgi_sc.pkl\", \"rb\") as f:\n",
    "        scs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Compute Laplacians an Sp Heats</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets = 21\n"
     ]
    }
   ],
   "source": [
    "N_data = len(names_hg)\n",
    "print('Number of datasets =', N_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/martamorandini/Desktop/projects/Simplicial_Renormalization/higher_order_LRG/HOLR/Experiments_setups/../Functions/renormalize.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  mu[i] = 1 / np.sum(np.exp(-tau * (D - D[i])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n",
      "03\n",
      "04\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [01:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(l) \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m## Configuration model\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m L \u001b[38;5;241m=\u001b[39m XO_laplacian_hg(sc, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(l[\u001b[38;5;241m0\u001b[39m]), l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(l[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse:\n\u001b[1;32m     53\u001b[0m     D,__ \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigsh(L\u001b[38;5;241m.\u001b[39masfptype(),k \u001b[38;5;241m=\u001b[39m num_eigs, which \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m, in \u001b[0;36mXO_laplacian_hg\u001b[0;34m(sc, k, l)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mXO_laplacian_hg\u001b[39m(sc,k,l):\n\u001b[0;32m---> 51\u001b[0m     A \u001b[38;5;241m=\u001b[39m adjacency_of_order_hg(sc,k,l)\n\u001b[1;32m     52\u001b[0m     K \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(A, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     53\u001b[0m     L \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(K) \u001b[38;5;241m-\u001b[39m A\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36madjacency_of_order_hg\u001b[0;34m(sc, k, l)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nk):\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,nk):\n\u001b[0;32m---> 17\u001b[0m             intersection \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mset\u001b[39m(diff_units[i,:]) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(diff_units[j,:]))\n\u001b[1;32m     18\u001b[0m             adj[i,j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(intersection)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SP_HEATS = {}\n",
    "for name in tqdm(names_hg):\n",
    "    #select dataset\n",
    "    sc = scs[name]\n",
    "    smax = 0\n",
    "    for i in range(5):\n",
    "        if sc[f'n{i}'] != 0:\n",
    "            smax +=1\n",
    "    # Define strings which specify the cross-order Laplacians to consider\n",
    "    laplacians_types = laplacians(smax - 1)\n",
    "\n",
    "    sparse = False\n",
    "    num_eigs = 500\n",
    "\n",
    "    C_curves = {l : [] for l in laplacians_types}\n",
    "    taumin = -3 # Heat curve starts from 10**taumin\n",
    "    taumax = 5 # Heat curve ends at 10**taumax\n",
    "    ntau = 200 # Number of taus to consider in the interval\n",
    "    for idl,l in tqdm(enumerate(laplacians_types)):    \n",
    "        ## Configuration model\n",
    "        L = XO_laplacian_hg(sc, k=int(l[0]), l=int(l[1]))\n",
    "            \n",
    "        if sparse:\n",
    "            D,__ = scipy.sparse.linalg.eigsh(L.asfptype(),k = num_eigs, which = \"SM\")\n",
    "            D = np.append(D,1000000*np.ones(L.shape[0]-num_eigs),axis=0)\n",
    "        else:\n",
    "            D,__ = np.linalg.eigh(L)\n",
    "            D = np.abs(D)\n",
    "        entropic_susceptibility,tau_space,__  = renormalize.compute_entropic_C(D,taumin,taumax,ntau)\n",
    "        C_curves[l] = entropic_susceptibility\n",
    "    SP_HEATS[name] = C_curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save specific heats for the hypergraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Experiments_results/xgi/xgi_spheats.pkl', \"wb\") as f:\n",
    "    pickle.dump(SP_HEATS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import specific heats for null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Experiments_results/xgi/xgi_spheats_cm.pkl', \"rb\") as f:\n",
    "    SP_HEATS_CM = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in tqdm(['congress-bills', 'email-eu']):\n",
    "    #select dataset\n",
    "    sc = scs[name]\n",
    "    smax = 0\n",
    "    for i in range(5):\n",
    "        if sc[f'n{i}'] != 0:\n",
    "            smax +=1\n",
    "    \n",
    "    NULL_MODEL = True\n",
    "    nrep = 3\n",
    "    if NULL_MODEL:\n",
    "        G = nx.from_edgelist(sc[\"edges\"])\n",
    "\n",
    "    sparse = False\n",
    "    num_eigs = 500\n",
    "\n",
    "    # Define strings which specify the cross-order Laplacians to consider\n",
    "    laplacians_types = laplacians(smax - 1)\n",
    "\n",
    "    C_curves = {l : [] for l in laplacians_types}\n",
    "    taumin = -3 # Heat curve starts from 10**taumin\n",
    "    taumax = 5 # Heat curve ends at 10**taumax\n",
    "    ntau = 200 # Number of taus to consider in the interval\n",
    "\n",
    "    if NULL_MODEL:\n",
    "        for idl,l in tqdm(enumerate(laplacians_types)): \n",
    "            A = adjacency_of_order_hg(sc, k=int(l[0]), l=int(l[1]))\n",
    "            for n in tqdm(range(nrep)):\n",
    "                # # Configuration model\n",
    "                Gcm = nx.Graph(A)\n",
    "                Gcm = nx.configuration_model([val for (__, val) in Gcm.degree()])\n",
    "                L = XO_laplacian_hg(sc, k=int(l[0]), l=int(l[1]))\n",
    "                \n",
    "                if sparse:\n",
    "                    D,__ = scipy.sparse.linalg.eigsh(L.asfptype(),k = num_eigs, which = \"SM\")\n",
    "                    D = np.append(D,1000000*np.ones(L.shape[0]-num_eigs),axis=0)\n",
    "                else:\n",
    "                    D,__ = np.linalg.eigh(L)\n",
    "                    D = np.abs(D)\n",
    "                entropic_susceptibility,tau_space,__  = renormalize.compute_entropic_C(D,taumin,taumax,ntau)\n",
    "                C_curves[l].append(entropic_susceptibility)\n",
    "\n",
    "\n",
    "    else:\n",
    "        for idl,l in tqdm(enumerate(laplacians_types)):    \n",
    "            ## Configuration model\n",
    "            L = XO_laplacian_hg(sc, k=int(l[0]), l=int(l[1]))\n",
    "            \n",
    "            if sparse:\n",
    "                D,__ = scipy.sparse.linalg.eigsh(L.asfptype(),k = num_eigs, which = \"SM\")\n",
    "                D = np.append(D,1000000*np.ones(L.shape[0]-num_eigs),axis=0)\n",
    "            else:\n",
    "                D,__ = np.linalg.eigh(L)\n",
    "                D = np.abs(D)\n",
    "            entropic_susceptibility,tau_space,__  = renormalize.compute_entropic_C(D,taumin,taumax,ntau)\n",
    "            C_curves[l] = entropic_susceptibility\n",
    "    SP_HEATS_CM[name] = C_curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Experiments_results/xgi/xgi_spheats_cm.pkl', \"wb\") as f:\n",
    "    pickle.dump(SP_HEAT_CM, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
